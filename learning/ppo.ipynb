{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "4bdf8df7",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "import random\n",
                "import time\n",
                "from dataclasses import dataclass\n",
                "import sys\n",
                "\n",
                "import gymnasium as gym\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.distributions.normal import Normal\n",
                "\n",
                "from agents.networks import ConvNet_StackedFrames\n",
                "from env_wrapper import ProcessedFrame, FrameStack, ActionRemapWrapper\n",
                "\n",
                "import wandb\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables from .env file\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "411e9ab0",
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_car_racing_env():\n",
                "    env = gym.make(\"CarRacing-v3\", continuous=True) \n",
                "    env = ProcessedFrame(env)\n",
                "    env = ActionRemapWrapper(env)\n",
                "    env = FrameStack(env, num_frames=4, skip_frames=2) # 2 frames stacked, skip 2\n",
                "    return env"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "0765bf25",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Agent(nn.Module):\n",
                "    def __init__(self, envs):\n",
                "        super().__init__()\n",
                "        # Matches CustomPPORLModule architecture\n",
                "        self.convnet = ConvNet_StackedFrames(num_frames=4)\n",
                "        \n",
                "        # ConvNet output: 256 channels * 4 * 4 spatial = 4096\n",
                "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
                "        self.fc2 = nn.Linear(512, 64)\n",
                "        \n",
                "        # Initialize FC weights\n",
                "        layer_init(self.fc1, std=np.sqrt(2))\n",
                "        layer_init(self.fc2, std=np.sqrt(2))\n",
                "        \n",
                "        # Policy Heads\n",
                "        # Steering: [-1, 1]\n",
                "        self.steering_mean = layer_init(nn.Linear(64, 1), std=0.01)\n",
                "        self.steering_log_std = layer_init(nn.Linear(64, 1), std=0.01)\n",
                "\n",
                "        # Gas: [-1, 1] -> [0, 1] (remapped)\n",
                "        self.gas_mean = layer_init(nn.Linear(64, 1), std=0.01, bias_const=-2.0)\n",
                "        self.gas_log_std = layer_init(nn.Linear(64, 1), std=0.01)\n",
                "\n",
                "        # Brake: [-1, 1] -> [0, 1] (remapped)\n",
                "        self.brake_mean = layer_init(nn.Linear(64, 1), std=0.01, bias_const=-2.0)\n",
                "        self.brake_log_std = layer_init(nn.Linear(64, 1), std=0.01)\n",
                "\n",
                "        # Value Head\n",
                "        self.vf_head = layer_init(nn.Linear(64, 1), std=1)\n",
                "\n",
                "        # Log std bounds\n",
                "        self.LOG_STD_MIN = -20\n",
                "        self.LOG_STD_MAX = 2\n",
                "\n",
                "    def get_features(self, x):\n",
                "        x = self.convnet(x)\n",
                "        x = x.reshape(x.size(0), -1)\n",
                "        x = torch.relu(self.fc1(x))\n",
                "        x = torch.relu(self.fc2(x))\n",
                "        return x\n",
                "\n",
                "    def get_value(self, x):\n",
                "        features = self.get_features(x)\n",
                "        return self.vf_head(features)\n",
                "\n",
                "    def get_action_and_value(self, x, action=None):\n",
                "        features = self.get_features(x)\n",
                "        \n",
                "        # Value\n",
                "        value = self.vf_head(features)\n",
                "\n",
                "        # Policy Heads - All use Tanh\n",
                "        steering_mean = torch.tanh(self.steering_mean(features))\n",
                "        gas_mean = torch.tanh(self.gas_mean(features))\n",
                "        brake_mean = torch.tanh(self.brake_mean(features))\n",
                "        \n",
                "        # Log Stds\n",
                "        steering_log_std = torch.clamp(self.steering_log_std(features), self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
                "        gas_log_std = torch.clamp(self.gas_log_std(features), self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
                "        brake_log_std = torch.clamp(self.brake_log_std(features), self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
                "        \n",
                "        # Concatenate\n",
                "        means = torch.cat([steering_mean, gas_mean, brake_mean], dim=1)\n",
                "        log_stds = torch.cat([steering_log_std, gas_log_std, brake_log_std], dim=1)\n",
                "        stds = torch.exp(log_stds)\n",
                "        \n",
                "        probs = Normal(means, stds)\n",
                "        if action is None:\n",
                "            action = probs.sample()\n",
                "            action = torch.clamp(action, -1.0, 1.0)  # Clamp actions to valid range\n",
                "            \n",
                "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), value\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "be7aa375",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/homes/vk545/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
                        "  from pkg_resources import resource_stream, resource_exists\n"
                    ]
                }
            ],
            "source": [
                "# Hyperparameters\n",
                "exp_name = \"ppo_car\"\n",
                "wandb_project_name = \"rl-training\"\n",
                "wandb_entity = None\n",
                "capture_video = False\n",
                "\n",
                "env_id = \"CarRacing-v3\"\n",
                "total_timesteps = 1000000\n",
                "learning_rate = 2.5e-4\n",
                "num_envs = 1\n",
                "num_steps = 128\n",
                "anneal_lr = True\n",
                "gamma = 0.99\n",
                "gae_lambda = 0.95\n",
                "num_minibatches = 4\n",
                "update_epochs = 4\n",
                "norm_adv = True\n",
                "clip_coef = 0.2\n",
                "clip_vloss = True\n",
                "ent_coef = 0.01\n",
                "vf_coef = 0.5\n",
                "max_grad_norm = 0.5\n",
                "target_kl = None\n",
                "\n",
                "batch_size = int(num_envs * num_steps)\n",
                "minibatch_size = int(batch_size // num_minibatches)\n",
                "num_iterations = total_timesteps // batch_size\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# Environment setup\n",
                "env = make_car_racing_env()\n",
                "\n",
                "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
                "    torch.nn.init.orthogonal_(layer.weight, std)\n",
                "    torch.nn.init.constant_(layer.bias, bias_const)\n",
                "    return layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "abc8f86a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malienpenguin\u001b[0m (\u001b[33malienpenguin-inc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.23.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/mnt/vurm/homes/homes/vk545/RL/wandb/run-20251216_124720-xw0kphfh</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/alienpenguin-inc/rl-training/runs/xw0kphfh' target=\"_blank\">ppo_car</a></strong> to <a href='https://wandb.ai/alienpenguin-inc/rl-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/alienpenguin-inc/rl-training' target=\"_blank\">https://wandb.ai/alienpenguin-inc/rl-training</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/alienpenguin-inc/rl-training/runs/xw0kphfh' target=\"_blank\">https://wandb.ai/alienpenguin-inc/rl-training/runs/xw0kphfh</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [agents] in use.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alienpenguin-inc/rl-training/runs/xw0kphfh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
                        ],
                        "text/plain": [
                            "<wandb.sdk.wandb_run.Run at 0x7b9403967a10>"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "wandb.login()\n",
                "wandb.init(\n",
                "    project=wandb_project_name,\n",
                "    name=exp_name,\n",
                "    config={\n",
                "        \"env_id\": env_id,\n",
                "        \"total_timesteps\": total_timesteps,\n",
                "        \"learning_rate\": learning_rate,\n",
                "        \"num_envs\": num_envs,\n",
                "        \"num_steps\": num_steps,\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "05fec1c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "agent = Agent(env).to(device)\n",
                "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "a55574b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ALGO Logic: Storage setup\n",
                "# Note: obs shape is (num_steps, num_envs, C, H, W). Even with num_envs=1.\n",
                "obs_shape = (num_steps, num_envs) + env.observation_space.shape\n",
                "obs = torch.zeros(obs_shape).to(device)\n",
                "\n",
                "actions = torch.zeros((num_steps, num_envs) + env.action_space.shape).to(device)\n",
                "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
                "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
                "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
                "values = torch.zeros((num_steps, num_envs)).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "35515912",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Iteration 1: Reward=0.00 Policy Loss=-0.0005 Value Loss=1.1716 KL=0.0000\n",
                        "Iteration 2: Reward=0.00 Policy Loss=-0.0019 Value Loss=0.6490 KL=0.0000\n",
                        "Iteration 3: Reward=0.00 Policy Loss=-0.0009 Value Loss=0.7339 KL=0.0002\n",
                        "Iteration 4: Reward=0.00 Policy Loss=-0.0071 Value Loss=0.5903 KL=0.0003\n",
                        "Iteration 5: Reward=0.00 Policy Loss=0.0028 Value Loss=0.5813 KL=0.0002\n",
                        "Iteration 6: Reward=0.00 Policy Loss=0.0025 Value Loss=0.7276 KL=0.0002\n",
                        "Iteration 7: Reward=0.00 Policy Loss=-0.0086 Value Loss=0.5169 KL=0.0009\n",
                        "Iteration 8: Reward=-40.77 Policy Loss=-0.0014 Value Loss=0.5484 KL=0.0005\n",
                        "Iteration 9: Reward=-40.77 Policy Loss=-0.0011 Value Loss=0.4760 KL=0.0006\n",
                        "Iteration 10: Reward=-40.77 Policy Loss=-0.0112 Value Loss=0.5660 KL=0.0017\n",
                        "Iteration 11: Reward=-40.77 Policy Loss=-0.0071 Value Loss=0.3546 KL=0.0021\n",
                        "Iteration 12: Reward=-40.77 Policy Loss=-0.0063 Value Loss=0.5613 KL=0.0013\n",
                        "Iteration 13: Reward=-40.77 Policy Loss=-0.0128 Value Loss=0.4939 KL=0.0021\n",
                        "Iteration 14: Reward=-40.77 Policy Loss=-0.0074 Value Loss=0.4117 KL=0.0143\n",
                        "Iteration 15: Reward=-40.77 Policy Loss=-0.0116 Value Loss=0.2910 KL=0.0140\n",
                        "Iteration 16: Reward=-70.78 Policy Loss=-0.0025 Value Loss=0.7761 KL=0.0006\n",
                        "Iteration 17: Reward=-70.78 Policy Loss=-0.0011 Value Loss=0.2781 KL=0.0031\n",
                        "Iteration 18: Reward=-70.78 Policy Loss=0.0016 Value Loss=0.4295 KL=0.0006\n",
                        "Iteration 19: Reward=-70.78 Policy Loss=0.0031 Value Loss=0.5229 KL=0.0007\n",
                        "Iteration 20: Reward=-70.78 Policy Loss=-0.0028 Value Loss=0.5636 KL=0.0023\n",
                        "Iteration 21: Reward=-70.78 Policy Loss=-0.0219 Value Loss=0.5315 KL=0.0055\n",
                        "Iteration 22: Reward=-70.78 Policy Loss=0.0025 Value Loss=0.4393 KL=0.0013\n",
                        "Iteration 23: Reward=-70.78 Policy Loss=-0.0087 Value Loss=0.5370 KL=0.0077\n",
                        "Iteration 24: Reward=-45.76 Policy Loss=0.0019 Value Loss=1.4723 KL=0.0040\n",
                        "Iteration 25: Reward=-45.76 Policy Loss=-0.0034 Value Loss=0.4746 KL=0.0017\n",
                        "Iteration 26: Reward=-45.76 Policy Loss=-0.0020 Value Loss=0.4050 KL=0.0020\n",
                        "Iteration 27: Reward=-45.76 Policy Loss=0.0016 Value Loss=0.3649 KL=0.0022\n",
                        "Iteration 28: Reward=-45.76 Policy Loss=-0.0060 Value Loss=0.2693 KL=0.0029\n",
                        "Iteration 29: Reward=-45.76 Policy Loss=-0.0007 Value Loss=0.2256 KL=0.0006\n",
                        "Iteration 30: Reward=-45.76 Policy Loss=0.0057 Value Loss=0.2115 KL=0.0041\n",
                        "Iteration 31: Reward=-45.76 Policy Loss=-0.0066 Value Loss=0.1888 KL=0.0030\n",
                        "Iteration 32: Reward=-58.20 Policy Loss=-0.0042 Value Loss=1.0906 KL=0.0002\n",
                        "Iteration 33: Reward=-58.20 Policy Loss=-0.0007 Value Loss=1.2710 KL=0.0006\n",
                        "Iteration 34: Reward=-58.20 Policy Loss=0.0021 Value Loss=0.7109 KL=0.0001\n",
                        "Iteration 35: Reward=-58.20 Policy Loss=-0.0157 Value Loss=0.4499 KL=0.0077\n",
                        "Iteration 36: Reward=-58.20 Policy Loss=0.0013 Value Loss=0.1723 KL=0.0077\n",
                        "Iteration 37: Reward=-58.20 Policy Loss=-0.0217 Value Loss=0.1618 KL=0.0054\n",
                        "Iteration 38: Reward=-58.20 Policy Loss=-0.0076 Value Loss=0.1195 KL=0.0081\n",
                        "Iteration 39: Reward=-58.20 Policy Loss=0.0021 Value Loss=0.1166 KL=0.0080\n",
                        "Iteration 40: Reward=-57.36 Policy Loss=0.0023 Value Loss=0.3912 KL=0.0014\n",
                        "Iteration 41: Reward=-57.36 Policy Loss=-0.0019 Value Loss=0.6709 KL=0.0006\n",
                        "Iteration 42: Reward=-57.36 Policy Loss=-0.0003 Value Loss=0.3787 KL=0.0008\n",
                        "Iteration 43: Reward=-57.36 Policy Loss=0.0017 Value Loss=0.2892 KL=0.0035\n",
                        "Iteration 44: Reward=-57.36 Policy Loss=-0.0055 Value Loss=0.0972 KL=0.0018\n",
                        "Iteration 45: Reward=-57.36 Policy Loss=0.0010 Value Loss=0.0739 KL=0.0071\n",
                        "Iteration 46: Reward=-57.36 Policy Loss=-0.0009 Value Loss=0.0505 KL=0.0117\n",
                        "Iteration 47: Reward=-68.55 Policy Loss=-0.0249 Value Loss=1.2520 KL=0.0071\n",
                        "Iteration 48: Reward=-68.55 Policy Loss=-0.0066 Value Loss=1.1252 KL=0.0131\n",
                        "Iteration 49: Reward=-68.55 Policy Loss=-0.0213 Value Loss=0.0422 KL=0.0408\n",
                        "Iteration 50: Reward=-68.55 Policy Loss=-0.0091 Value Loss=0.0246 KL=0.0068\n",
                        "Iteration 51: Reward=-68.55 Policy Loss=-0.0229 Value Loss=0.0112 KL=0.0303\n",
                        "Iteration 52: Reward=-68.55 Policy Loss=-0.0040 Value Loss=0.0063 KL=0.0285\n",
                        "Iteration 53: Reward=-68.55 Policy Loss=0.0031 Value Loss=0.0099 KL=0.0025\n",
                        "Iteration 54: Reward=-68.55 Policy Loss=0.0135 Value Loss=0.0315 KL=0.0192\n",
                        "Iteration 55: Reward=-75.44 Policy Loss=-0.0114 Value Loss=1.9019 KL=0.0092\n",
                        "Iteration 56: Reward=-75.44 Policy Loss=0.0025 Value Loss=0.5026 KL=0.0009\n",
                        "Iteration 57: Reward=-75.44 Policy Loss=-0.0020 Value Loss=0.4207 KL=0.0151\n",
                        "Iteration 58: Reward=-75.44 Policy Loss=-0.0119 Value Loss=0.9924 KL=0.0046\n",
                        "Iteration 59: Reward=-75.44 Policy Loss=-0.0039 Value Loss=0.2671 KL=0.0146\n",
                        "Iteration 60: Reward=-75.44 Policy Loss=-0.0025 Value Loss=0.0099 KL=0.0107\n",
                        "Iteration 61: Reward=-75.44 Policy Loss=-0.0068 Value Loss=0.0035 KL=0.0048\n",
                        "Iteration 62: Reward=-75.44 Policy Loss=-0.0015 Value Loss=0.0031 KL=0.0014\n",
                        "Iteration 63: Reward=-64.40 Policy Loss=-0.0004 Value Loss=2.5970 KL=0.0112\n",
                        "Iteration 64: Reward=-64.40 Policy Loss=-0.0007 Value Loss=0.8669 KL=0.0000\n",
                        "Iteration 65: Reward=-64.40 Policy Loss=-0.0113 Value Loss=0.0077 KL=0.0122\n",
                        "Iteration 66: Reward=-64.40 Policy Loss=0.0007 Value Loss=0.0082 KL=0.0054\n",
                        "Iteration 67: Reward=-64.40 Policy Loss=0.0044 Value Loss=0.0239 KL=0.0043\n",
                        "Iteration 68: Reward=-64.40 Policy Loss=-0.0081 Value Loss=0.0081 KL=0.0110\n",
                        "Iteration 69: Reward=-64.40 Policy Loss=-0.0016 Value Loss=0.0013 KL=0.0014\n",
                        "Iteration 70: Reward=-64.40 Policy Loss=-0.0202 Value Loss=0.0011 KL=0.0292\n",
                        "Iteration 71: Reward=-72.13 Policy Loss=0.0031 Value Loss=6.4539 KL=0.0154\n",
                        "Iteration 72: Reward=-72.13 Policy Loss=0.0096 Value Loss=1.9812 KL=0.0040\n",
                        "Iteration 73: Reward=-72.13 Policy Loss=0.0102 Value Loss=1.1813 KL=0.0257\n",
                        "Iteration 74: Reward=-72.13 Policy Loss=-0.0231 Value Loss=0.0070 KL=0.0110\n",
                        "Iteration 75: Reward=-72.13 Policy Loss=-0.0025 Value Loss=0.0066 KL=0.0008\n",
                        "Iteration 76: Reward=-72.13 Policy Loss=0.0061 Value Loss=0.0098 KL=0.0132\n",
                        "Iteration 77: Reward=-72.13 Policy Loss=-0.0029 Value Loss=0.0047 KL=0.0076\n",
                        "Iteration 78: Reward=-72.13 Policy Loss=-0.0145 Value Loss=0.0046 KL=0.0164\n",
                        "Iteration 79: Reward=-43.28 Policy Loss=0.0453 Value Loss=4.2434 KL=0.0111\n",
                        "Iteration 80: Reward=-43.28 Policy Loss=-0.0032 Value Loss=1.7036 KL=0.0008\n",
                        "Iteration 81: Reward=-43.28 Policy Loss=-0.0011 Value Loss=0.0093 KL=0.0002\n",
                        "Iteration 82: Reward=-43.28 Policy Loss=-0.0106 Value Loss=0.0205 KL=0.0005\n",
                        "Iteration 83: Reward=-43.28 Policy Loss=-0.0021 Value Loss=0.0048 KL=0.0036\n",
                        "Iteration 84: Reward=-43.28 Policy Loss=-0.0078 Value Loss=0.0028 KL=0.0043\n",
                        "Iteration 85: Reward=-43.28 Policy Loss=0.0012 Value Loss=0.0059 KL=0.0040\n",
                        "Iteration 86: Reward=-67.39 Policy Loss=-0.0005 Value Loss=4.1184 KL=0.0010\n",
                        "Iteration 87: Reward=-67.39 Policy Loss=-0.0113 Value Loss=0.9557 KL=0.0030\n",
                        "Iteration 88: Reward=-67.39 Policy Loss=0.0001 Value Loss=0.0044 KL=0.0001\n",
                        "Iteration 89: Reward=-67.39 Policy Loss=-0.0146 Value Loss=0.0016 KL=0.0053\n",
                        "Iteration 90: Reward=-67.39 Policy Loss=0.0219 Value Loss=0.0018 KL=0.0019\n",
                        "Iteration 91: Reward=-67.39 Policy Loss=-0.0023 Value Loss=0.0029 KL=0.0003\n",
                        "Iteration 92: Reward=-67.39 Policy Loss=-0.0058 Value Loss=0.0072 KL=0.0020\n",
                        "Iteration 93: Reward=-67.39 Policy Loss=0.0000 Value Loss=2.0457 KL=0.0003\n",
                        "Iteration 94: Reward=-56.23 Policy Loss=0.0092 Value Loss=3.8357 KL=0.0047\n",
                        "Iteration 95: Reward=-56.23 Policy Loss=-0.0029 Value Loss=2.2782 KL=0.0027\n",
                        "Iteration 96: Reward=-56.23 Policy Loss=-0.0093 Value Loss=0.0026 KL=0.0030\n",
                        "Iteration 97: Reward=-56.23 Policy Loss=0.0040 Value Loss=0.0139 KL=0.0007\n",
                        "Iteration 98: Reward=-56.23 Policy Loss=-0.0076 Value Loss=0.0029 KL=0.0089\n",
                        "Iteration 99: Reward=-56.23 Policy Loss=-0.0051 Value Loss=0.0005 KL=0.0018\n",
                        "Iteration 100: Reward=-56.23 Policy Loss=-0.0127 Value Loss=0.0007 KL=0.0072\n",
                        "Iteration 101: Reward=-56.23 Policy Loss=-0.0338 Value Loss=0.2279 KL=0.0333\n",
                        "Iteration 102: Reward=-71.83 Policy Loss=-0.1742 Value Loss=6.4130 KL=0.0376\n",
                        "Iteration 103: Reward=-71.83 Policy Loss=-0.0011 Value Loss=2.0374 KL=0.0015\n",
                        "Iteration 104: Reward=-71.83 Policy Loss=-0.0138 Value Loss=0.0118 KL=0.0011\n",
                        "Iteration 105: Reward=-71.83 Policy Loss=-0.0039 Value Loss=2.8065 KL=0.0019\n",
                        "Iteration 106: Reward=-71.83 Policy Loss=-0.0079 Value Loss=0.0030 KL=0.0277\n",
                        "Iteration 107: Reward=-71.83 Policy Loss=0.0018 Value Loss=8.5430 KL=0.0061\n",
                        "Iteration 108: Reward=-71.83 Policy Loss=0.0033 Value Loss=0.0021 KL=0.0017\n",
                        "Iteration 109: Reward=-71.83 Policy Loss=-0.0081 Value Loss=0.0104 KL=0.0008\n",
                        "Iteration 110: Reward=-29.58 Policy Loss=-0.0033 Value Loss=4.8861 KL=0.0075\n",
                        "Iteration 111: Reward=-29.58 Policy Loss=-0.0068 Value Loss=0.0655 KL=0.0010\n",
                        "Iteration 112: Reward=-29.58 Policy Loss=-0.0049 Value Loss=0.0164 KL=0.0001\n",
                        "Iteration 113: Reward=-29.58 Policy Loss=-0.0079 Value Loss=0.0499 KL=0.0005\n",
                        "Iteration 114: Reward=-29.58 Policy Loss=0.0261 Value Loss=3.1994 KL=0.0037\n",
                        "Iteration 115: Reward=-29.58 Policy Loss=-0.0069 Value Loss=0.0210 KL=0.0229\n",
                        "Iteration 116: Reward=-29.58 Policy Loss=0.0028 Value Loss=0.5363 KL=0.0006\n",
                        "Iteration 117: Reward=-29.58 Policy Loss=-0.0020 Value Loss=0.3717 KL=0.0380\n",
                        "Iteration 118: Reward=-60.29 Policy Loss=-0.0119 Value Loss=4.2758 KL=0.0263\n",
                        "Iteration 119: Reward=-60.29 Policy Loss=-0.0150 Value Loss=0.0095 KL=0.0034\n",
                        "Iteration 120: Reward=-111.38 Policy Loss=-0.0959 Value Loss=637.1869 KL=0.4220\n",
                        "Iteration 121: Reward=-111.38 Policy Loss=0.0272 Value Loss=0.0496 KL=0.0071\n",
                        "Iteration 122: Reward=-111.38 Policy Loss=-0.0056 Value Loss=4.5290 KL=0.0007\n",
                        "Iteration 123: Reward=-111.38 Policy Loss=0.0131 Value Loss=18.1367 KL=0.0013\n",
                        "Iteration 124: Reward=-111.38 Policy Loss=-0.0006 Value Loss=0.0092 KL=0.0047\n",
                        "Iteration 125: Reward=-111.38 Policy Loss=-0.0188 Value Loss=0.0401 KL=0.0031\n",
                        "Iteration 126: Reward=-111.38 Policy Loss=0.0084 Value Loss=0.0544 KL=0.0293\n",
                        "Iteration 127: Reward=-111.38 Policy Loss=-0.0549 Value Loss=0.1671 KL=0.0046\n",
                        "Iteration 128: Reward=-15.71 Policy Loss=0.0059 Value Loss=3.4802 KL=0.0050\n",
                        "Iteration 129: Reward=-15.71 Policy Loss=0.0033 Value Loss=2.0441 KL=0.0019\n",
                        "Iteration 130: Reward=-15.71 Policy Loss=-0.0010 Value Loss=1.1836 KL=0.0002\n",
                        "Iteration 131: Reward=-15.71 Policy Loss=-0.0395 Value Loss=0.1901 KL=0.0173\n",
                        "Iteration 132: Reward=-113.98 Policy Loss=-0.0281 Value Loss=420.3500 KL=0.0566\n",
                        "Iteration 133: Reward=-113.98 Policy Loss=0.0036 Value Loss=12.9085 KL=0.0121\n",
                        "Iteration 134: Reward=-113.98 Policy Loss=-0.0037 Value Loss=0.3430 KL=0.0051\n",
                        "Iteration 135: Reward=-113.98 Policy Loss=0.0055 Value Loss=0.8967 KL=0.0014\n",
                        "Iteration 136: Reward=-113.98 Policy Loss=-0.0260 Value Loss=0.0334 KL=0.0111\n",
                        "Iteration 137: Reward=-113.98 Policy Loss=-0.0466 Value Loss=0.0027 KL=0.0113\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    123\u001b[39m loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n\u001b[32m    125\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n\u001b[32m    128\u001b[39m optimizer.step()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7b94c1996cc0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b9397d5c620, execution_count=8 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7b941c5399a0, raw_cell=\"global_step = 0\n",
                        "start_time = time.time()\n",
                        "last_epis..\" transformed_cell=\"global_step = 0\n",
                        "start_time = time.time()\n",
                        "last_epis..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227061726b696e227d/homes/vk545/RL/ppo.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "ConnectionResetError",
                    "evalue": "Connection lost",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:603\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:820\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    819\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Neuralese/miniconda3/envs/rl/lib/python3.12/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
                        "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
                    ]
                }
            ],
            "source": [
                "global_step = 0\n",
                "start_time = time.time()\n",
                "last_episode_return = 0\n",
                "running_return = 0\n",
                "\n",
                "# Initial reset\n",
                "next_obs, _ = env.reset()\n",
                "next_obs = torch.Tensor(next_obs).to(device).unsqueeze(0)  # Shape: (1, 4, 84, 96)\n",
                "next_done = torch.zeros(num_envs).to(device)\n",
                "\n",
                "for iteration in range(1, num_iterations + 1):\n",
                "    # Annealing the rate if instructed to do so.\n",
                "    if anneal_lr:\n",
                "        frac = 1.0 - (iteration - 1.0) / num_iterations\n",
                "        lrnow = frac * learning_rate\n",
                "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
                "\n",
                "    for step in range(0, num_steps):\n",
                "        global_step += num_envs\n",
                "        obs[step] = next_obs \n",
                "        dones[step] = next_done\n",
                "\n",
                "        # ALGO LOGIC: action logic\n",
                "        with torch.no_grad():\n",
                "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
                "            \n",
                "            values[step] = value.flatten()\n",
                "        \n",
                "        actions[step] = action\n",
                "        logprobs[step] = logprob\n",
                "\n",
                "        # TRY NOT TO MODIFY: execute the game and log data.\n",
                "        real_action = action[0].cpu().numpy()\n",
                "        next_obs, reward, terminated, truncated, info = env.step(real_action)\n",
                "        done = terminated or truncated\n",
                "        \n",
                "        running_return += reward  # Manual reward tracking\n",
                "        \n",
                "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
                "        \n",
                "        # Handle Reset\n",
                "        if done:\n",
                "            next_obs, _ = env.reset()\n",
                "            next_done = torch.ones(num_envs).to(device)\n",
                "            last_episode_return = running_return\n",
                "            running_return = 0\n",
                "        else:\n",
                "            next_done = torch.zeros(num_envs).to(device)\n",
                "            \n",
                "        next_obs = torch.Tensor(next_obs).to(device).unsqueeze(0) # Ensure (1, C, H, W)\n",
                "\n",
                "    # bootstrap value if not done\n",
                "    with torch.no_grad():\n",
                "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
                "        advantages = torch.zeros_like(rewards).to(device)\n",
                "        lastgaelam = 0\n",
                "        for t in reversed(range(num_steps)):\n",
                "            if t == num_steps - 1:\n",
                "                nextnonterminal = 1.0 - next_done\n",
                "                nextvalues = next_value\n",
                "            else:\n",
                "                nextnonterminal = 1.0 - dones[t + 1]\n",
                "                nextvalues = values[t + 1]\n",
                "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
                "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
                "        returns = advantages + values\n",
                "    \n",
                "    # flatten the batch\n",
                "    b_obs = obs.reshape((-1,) + env.observation_space.shape)\n",
                "    b_logprobs = logprobs.reshape(-1)\n",
                "    b_actions = actions.reshape((-1,) + env.action_space.shape)\n",
                "    b_advantages = advantages.reshape(-1)\n",
                "    b_returns = returns.reshape(-1)\n",
                "    b_values = values.reshape(-1)\n",
                "\n",
                "    # Optimizing the policy and value network\n",
                "    b_inds = np.arange(batch_size)\n",
                "    clipfracs = []\n",
                "    for epoch in range(update_epochs):\n",
                "        np.random.shuffle(b_inds)\n",
                "        for start in range(0, batch_size, minibatch_size):\n",
                "            end = start + minibatch_size\n",
                "            mb_inds = b_inds[start:end]\n",
                "\n",
                "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
                "            logratio = newlogprob - b_logprobs[mb_inds]\n",
                "            ratio = logratio.exp()\n",
                "\n",
                "            with torch.no_grad():\n",
                "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
                "                old_approx_kl = (-logratio).mean()\n",
                "                approx_kl = ((ratio - 1) - logratio).mean()\n",
                "                clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
                "\n",
                "            mb_advantages = b_advantages[mb_inds]\n",
                "            if norm_adv:\n",
                "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
                "\n",
                "            # Policy loss\n",
                "            pg_loss1 = -mb_advantages * ratio\n",
                "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
                "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
                "\n",
                "            # Value loss\n",
                "            newvalue = newvalue.view(-1)\n",
                "            if clip_vloss:\n",
                "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
                "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
                "                    newvalue - b_values[mb_inds],\n",
                "                    -clip_coef,\n",
                "                    clip_coef,\n",
                "                )\n",
                "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
                "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
                "                v_loss = 0.5 * v_loss_max.mean()\n",
                "            else:\n",
                "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
                "\n",
                "            entropy_loss = entropy.mean()\n",
                "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
                "            optimizer.step()\n",
                "\n",
                "        if target_kl is not None and approx_kl > target_kl:\n",
                "            break\n",
                "    \n",
                "    # Logging\n",
                "    wandb.log({\n",
                "        \"episode_reward\": last_episode_return,\n",
                "        \"losses/policy_loss\": pg_loss.item(),\n",
                "        \"losses/value_loss\": v_loss.item(),\n",
                "        \"losses/kl\": approx_kl.item(),\n",
                "        \"charts/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
                "        \"global_step\": global_step\n",
                "    })\n",
                "    \n",
                "    print(f\"Iteration {iteration}: Reward={last_episode_return:.2f} Policy Loss={pg_loss.item():.4f} Value Loss={v_loss.item():.4f} KL={approx_kl.item():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70cb24b4",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
